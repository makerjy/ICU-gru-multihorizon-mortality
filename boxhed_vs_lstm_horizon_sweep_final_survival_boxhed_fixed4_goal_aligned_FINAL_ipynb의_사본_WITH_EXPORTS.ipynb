{"cells":[{"cell_type":"markdown","id":"2c9e47ce","metadata":{"id":"2c9e47ce"},"source":["# BoXHED vs LSTM (누수 완화 버전: 미래 예측 + 이벤트 이후 제거)\n","\n","## 예측 정의(두 모델 동일 적용)\n","- **대상 단위**: `stay_id`\n","- **시간 축**: `t` (시간 인덱스)\n","- **원천 이벤트 컬럼**: `event`(우선) 또는 `delta`\n","- **미래 예측 라벨**: 각 시점 `t`에서 **(t, t+H] 구간(H=`HORIZON_HOURS`) 안에 이벤트가 1회라도 발생하면 1**, 아니면 0\n","  - 동시 시점(event at t)은 ‘예측’이 아니므로 라벨 계산에서 제외\n","- **이벤트 이후 구간 제거**: 각 stay에서 **첫 이벤트 시점 `first_event_t` 이후(t >= first_event_t) row는 제거**하여, 이벤트 이후 정보로 학습/평가하는 문제를 줄임\n","\n","## 평가(두 모델 동일 기준)\n","- row-level score(`row_score`)를 `t <= CUTOFF_HOURS` 구간에서 `AGG_MODE`로 stay-level로 집계\n","- stay-level 정답은 **`t <= (CUTOFF_HOURS + HORIZON_HOURS)` 내 실제 이벤트 발생 여부**\n","- threshold는 validation에서 **목표 recall(TARGET_RECALL) 이상 중 precision 최대**로 선택 후 test에 동일 적용\n"]},{"cell_type":"code","execution_count":1,"id":"362b2859","metadata":{"id":"362b2859","executionInfo":{"status":"ok","timestamp":1768635921633,"user_tz":-540,"elapsed":37501,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["# 환경 준비\n","!pip -q install -U \"numpy<2\"\n","!pip -q install -U boxhed\n","!pip -q install -U tensorflow\n","\n","import sys\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","\n","# numpy 2.x 호환 꼬임 방지\n","if not hasattr(np, \"cfloat\"):\n","    np.cfloat = np.complex128\n","\n","from sklearn.metrics import (\n","    roc_auc_score, average_precision_score,\n","    precision_recall_curve, confusion_matrix,\n","    precision_score, recall_score, f1_score, accuracy_score\n",")\n","from sklearn.preprocessing import StandardScaler\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","from boxhed.boxhed import boxhed\n"]},{"cell_type":"code","execution_count":2,"id":"0ec4b7b9","metadata":{"id":"0ec4b7b9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768635956213,"user_tz":-540,"elapsed":34572,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}},"outputId":"5a3014b7-9e5b-49d9-ac35-0634cdac13ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Loaded: (3987360, 15) (976560, 15) (1243080, 15)\n","Columns: 15\n"]}],"source":["# 데이터 로드\n","IN_COLAB = \"google.colab\" in sys.modules\n","DRIVE_SOURCE_FOLDER = \"/content/drive/MyDrive/mini/박소현/boxhed_io\"  # 필요 시 수정\n","\n","if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount(\"/content/drive\")\n","\n","base = Path(DRIVE_SOURCE_FOLDER)\n","train_path = base / \"train_df.csv\"\n","valid_path = base / \"valid_df.csv\"\n","test_path  = base / \"test_df.csv\"\n","\n","for p in [train_path, valid_path, test_path]:\n","    if not p.exists():\n","        raise FileNotFoundError(f\"파일이 없음: {p}\")\n","\n","train_df = pd.read_csv(train_path)\n","valid_df = pd.read_csv(valid_path)\n","test_df  = pd.read_csv(test_path)\n","\n","print(\"Loaded:\", train_df.shape, valid_df.shape, test_df.shape)\n","print(\"Columns:\", len(train_df.columns))\n"]},{"cell_type":"code","execution_count":3,"id":"49f102e1","metadata":{"id":"49f102e1","executionInfo":{"status":"ok","timestamp":1768635983812,"user_tz":-540,"elapsed":27579,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["# 공통 스키마/설정\n","ID_COL = \"stay_id\"\n","TIME_COL = \"t\"\n","EVENT_COL = \"event\" if \"event\" in train_df.columns else \"delta\"\n","WEIGHT_COL = \"sample_weight\" if \"sample_weight\" in train_df.columns else None\n","\n","# ===== 목표 정합 설정 =====\n","HORIZON_HOURS = 6          # 시점 t에서 (t, t+H] 이내 이벤트\n","DROP_AFTER_EVENT = True    # 이벤트 이후 관측 제거 (t >= first_event_t 제거)\n","CUTOFF_HOURS = 24\n","AGG_MODE = \"max\"           # {\"max\",\"mean\",\"last\"}\n","TARGET_RECALL = 0.80\n","\n","import numpy as np\n","import pandas as pd\n","\n","def add_future_label(df: pd.DataFrame, id_col=ID_COL, time_col=TIME_COL, event_col=EVENT_COL, horizon_hours=None):\n","    \"\"\"\n","    각 row(t)에서 (t, t+H] 이내 이벤트가 있으면 1.\n","    (동시시점 event는 예측이 아니므로 제외)\n","    \"\"\"\n","    out = df.copy()\n","    out[\"_future_label\"] = 0\n","    if horizon_hours is None:\n","        horizon_hours = HORIZON_HOURS\n","\n","    for sid, g in out.sort_values([id_col, time_col]).groupby(id_col, sort=False):\n","        t = g[time_col].to_numpy()\n","        e = g[event_col].to_numpy().astype(int)\n","        ev_times = t[e == 1]\n","        if ev_times.size == 0:\n","            continue\n","\n","        left = np.searchsorted(ev_times, t, side=\"right\")\n","        right = np.searchsorted(ev_times, t + horizon_hours, side=\"right\")\n","        out.loc[g.index, \"_future_label\"] = ((right - left) > 0).astype(int)\n","\n","    return out\n","\n","def add_label_observable_mask(df: pd.DataFrame, id_col=ID_COL, time_col=TIME_COL, label_col=\"_future_label\",\n","                              horizon_hours=None):\n","    \"\"\"\n","    우측 검열 처리:\n","    - label==1 이면 관측 가능(True)\n","    - label==0 이면, 최소 t+H까지 관측이 존재해야 관측 가능(True)\n","    \"\"\"\n","    out = df.copy()\n","    if horizon_hours is None:\n","        horizon_hours = HORIZON_HOURS\n","\n","    t_last = out.groupby(id_col)[time_col].max().rename(\"_t_last\")\n","    out = out.merge(t_last, on=id_col, how=\"left\")\n","    out[\"_label_observable\"] = (out[label_col] == 1) | (out[time_col] + horizon_hours <= out[\"_t_last\"])\n","    out = out.drop(columns=[\"_t_last\"])\n","    return out\n","\n","def drop_rows_after_first_event(df: pd.DataFrame, id_col=ID_COL, time_col=TIME_COL, event_col=EVENT_COL):\n","    \"\"\"첫 이벤트 시점 이상(t >= first_event_t) row 제거.\"\"\"\n","    out = df.copy()\n","    first_t = (\n","        out.loc[out[event_col].astype(int) == 1]\n","           .groupby(id_col)[time_col]\n","           .min()\n","    )\n","    out = out.merge(first_t.rename(\"_first_event_t\"), on=id_col, how=\"left\")\n","    keep = out[\"_first_event_t\"].isna() | (out[time_col] < out[\"_first_event_t\"])\n","    out = out.loc[keep].drop(columns=[\"_first_event_t\"])\n","    return out\n","\n","# 1) 미래 라벨 생성\n","train_df = add_future_label(train_df, horizon_hours=HORIZON_HOURS)\n","valid_df = add_future_label(valid_df, horizon_hours=HORIZON_HOURS)\n","test_df  = add_future_label(test_df,  horizon_hours=HORIZON_HOURS)\n","\n","# 2) 우측 검열 마스크 생성 후, 관측 불가능 row 제거(또는 weight=0 처리 가능)\n","train_df = add_label_observable_mask(train_df, horizon_hours=HORIZON_HOURS)\n","valid_df = add_label_observable_mask(valid_df, horizon_hours=HORIZON_HOURS)\n","test_df  = add_label_observable_mask(test_df,  horizon_hours=HORIZON_HOURS)\n","\n","train_df = train_df.loc[train_df[\"_label_observable\"]].copy()\n","valid_df = valid_df.loc[valid_df[\"_label_observable\"]].copy()\n","test_df  = test_df.loc[test_df[\"_label_observable\"]].copy()\n","\n","# 3) 이벤트 이후 정보 제거(누수 방지)\n","if DROP_AFTER_EVENT:\n","    train_df = drop_rows_after_first_event(train_df)\n","    valid_df = drop_rows_after_first_event(valid_df)\n","    test_df  = drop_rows_after_first_event(test_df)\n","\n","LABEL_COL = \"_future_label\"  # 모든 모델 공통 타깃(미래 H시간 라벨)\n","\n","# (옵션) 운영 가정: 학습을 cutoff로 자를지\n","APPLY_CUTOFF_TO_TRAIN = False\n","if APPLY_CUTOFF_TO_TRAIN:\n","    train_df = train_df.loc[train_df[TIME_COL] <= CUTOFF_HOURS].copy()\n"]},{"cell_type":"code","execution_count":4,"id":"bef110b6","metadata":{"id":"bef110b6","executionInfo":{"status":"ok","timestamp":1768635983836,"user_tz":-540,"elapsed":11,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["\n","# 공통 평가 유틸\n","from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n","\n","def safe_auc_ap(y, s):\n","    y = np.asarray(y).astype(int)\n","    s = np.asarray(s).astype(float)\n","    if len(np.unique(y)) < 2:\n","        return np.nan, np.nan\n","    return roc_auc_score(y, s), average_precision_score(y, s)\n","\n","def stay_level_from_row(df_long: pd.DataFrame, row_score: np.ndarray,\n","                        id_col=ID_COL, time_col=TIME_COL,\n","                        cutoff_hours=CUTOFF_HOURS,\n","                        agg=AGG_MODE,\n","                        label_col=LABEL_COL,\n","                        horizon_hours=None,\n","                        **kwargs):\n","    \"\"\"    Convert row-level scores to stay-level labels/scores under the SAME evaluation frame.\n","\n","    IMPORTANT:\n","      - stay-level ground truth is derived from `label_col` (e.g., _future_label),\n","        not from raw `event`, so that DROP_AFTER_EVENT does not zero-out positives.\n","      - aggregation is computed within `t <= cutoff_hours`.\n","    \"\"\"\n","    d = df_long.sort_values([id_col, time_col]).copy()\n","    d_cut = d.loc[d[time_col] <= cutoff_hours].copy()\n","    d_cut[\"_row_score\"] = row_score\n","\n","    if agg == \"max\":\n","        s_stay = d_cut.groupby(id_col)[\"_row_score\"].max()\n","    elif agg == \"mean\":\n","        s_stay = d_cut.groupby(id_col)[\"_row_score\"].mean()\n","    elif agg == \"last\":\n","        s_stay = d_cut.groupby(id_col)[\"_row_score\"].last()\n","    else:\n","        raise ValueError(f\"Unknown agg: {agg}\")\n","\n","    y_stay = d_cut.groupby(id_col)[label_col].max().astype(int)\n","\n","    common = s_stay.index.intersection(y_stay.index)\n","    return y_stay.loc[common].to_numpy(), s_stay.loc[common].to_numpy()\n","\n","def pick_threshold_by_recall_then_precision(y, s, target_recall=TARGET_RECALL):\n","    \"\"\"validation에서 target_recall 이상인 threshold 중 precision 최대 선택.\"\"\"\n","    precision, recall, thr = precision_recall_curve(y, s)\n","    # precision_recall_curve는 thr 길이가 (n-1)라서 정렬 맞추기\n","    # thr에 대해 recall/precision을 맞추기 위해 마지막 원소 제거\n","    precision = precision[:-1]\n","    recall = recall[:-1]\n","\n","    ok = recall >= target_recall\n","    if not np.any(ok):\n","        # 타겟 recall 달성 불가면 recall 최대인 지점 선택\n","        i = int(np.argmax(recall))\n","        return float(thr[i]), float(precision[i]), float(recall[i])\n","\n","    cand = np.where(ok)[0]\n","    # precision 최대, 동률이면 threshold 더 큰 것(보수적)\n","    best = cand[np.lexsort((-thr[cand], -precision[cand]))][0]\n","    return float(thr[best]), float(precision[best]), float(recall[best])\n","\n","def evaluate_split(name, df, row_score):\n","    y, s = stay_level_from_row(df, row_score)\n","    auc, ap = safe_auc_ap(y, s)\n","    return {\"split\": name, \"auc\": auc, \"ap\": ap, \"n_stay\": int(len(y))}\n"]},{"cell_type":"code","execution_count":5,"id":"1629948d","metadata":{"id":"1629948d","executionInfo":{"status":"ok","timestamp":1768635983854,"user_tz":-540,"elapsed":9,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["# =============================\n","# Helper functions (self-contained)\n","# =============================\n","\n","import numpy as np\n","\n","def stay_score_with_cutoff(df_long, row_score, cutoff_hours=CUTOFF_HOURS, agg=AGG_MODE):\n","    if len(row_score) != len(df_long):\n","        raise ValueError(f\"Length mismatch: row_score={len(row_score)} df={len(df_long)}\")\n","\n","    d = df_long.copy()\n","    d[\"_row_score\"] = np.asarray(row_score, dtype=np.float32)\n","    d = d.sort_values([ID_COL, TIME_COL])\n","    d_cut = d.loc[d[TIME_COL] <= cutoff_hours].copy()\n","\n","    if agg == \"max\":\n","        s_stay = d_cut.groupby(ID_COL)[\"_row_score\"].max()\n","    elif agg == \"mean\":\n","        s_stay = d_cut.groupby(ID_COL)[\"_row_score\"].mean()\n","    elif agg == \"last\":\n","        s_stay = d_cut.groupby(ID_COL)[\"_row_score\"].last()\n","    else:\n","        raise ValueError(f\"Unknown agg: {agg}\")\n","\n","    y_stay = d_cut.groupby(ID_COL)[LABEL_COL].max().astype(int)\n","    common = s_stay.index.intersection(y_stay.index)\n","    return y_stay.loc[common].to_numpy(), s_stay.loc[common].to_numpy()\n","\n","def apply_class_weight(w, y):\n","    y = np.asarray(y).astype(int)\n","    n_pos = (y == 1).sum()\n","    n_neg = (y == 0).sum()\n","    if n_pos == 0:\n","        raise ValueError(\"No positive labels in training.\")\n","    pos_mult = n_neg / n_pos\n","    w2 = np.ones_like(y, dtype=np.float32) if w is None else np.asarray(w, dtype=np.float32).copy()\n","    w2[y == 1] *= float(pos_mult)\n","    return w2\n","\n","def hazard_to_1d(hz):\n","    hz = np.asarray(hz)\n","    if hz.ndim == 1:\n","        return hz.astype(np.float32)\n","    hz = hz.reshape(hz.shape[0], -1)\n","    return hz.max(axis=1).astype(np.float32)\n","\n","def sigmoid(x):\n","    x = np.asarray(x, dtype=np.float32)\n","    x = np.clip(x, -50.0, 50.0)\n","    return 1.0 / (1.0 + np.exp(-x))\n","\n","def predict_row_score_boxhed_df(model, df_eval, feature_cols):\n","    \"\"\"\n","    BoXHED를 '미래 H시간 라벨' 분류기로 사용:\n","    - hazard 출력(스코어)을 sigmoid로 확률화 (단조 변환이므로 AUC/AP에 안정적)\n","    \"\"\"\n","    X = df_eval[feature_cols].to_numpy(dtype=np.float32)\n","    hz = hazard_to_1d(model.hazard(X))\n","    hz = np.nan_to_num(hz, nan=0.0, posinf=50.0, neginf=-50.0)\n","    p = sigmoid(hz).astype(np.float32)\n","    if len(p) != len(df_eval):\n","        raise RuntimeError(\"BoXHED row_score length mismatch\")\n","    return p\n"]},{"cell_type":"code","execution_count":7,"id":"18599eb5","metadata":{"id":"18599eb5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768636048425,"user_tz":-540,"elapsed":22,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}},"outputId":"360a966e-a96e-4fd2-ffd0-815cbd4082ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["n_features=12\n","['HeartRate_std_6h', 'GCS_Verbal', 'SpO2_measured', 'RespRate_std_6h', 'SysBP', 'GCS_Motor', 'GCS_Total_mean_6h', 'Temp_std_6h', 'pH', 'DiasBP_mean_6h', 'MeanBP', 'FiO2']\n","Index(['stay_id', 't', 'event', 'HeartRate_std_6h', 'GCS_Verbal',\n","       'SpO2_measured', 'RespRate_std_6h', 'SysBP', 'GCS_Motor',\n","       'GCS_Total_mean_6h', 'Temp_std_6h', 'pH', 'DiasBP_mean_6h', 'MeanBP',\n","       'FiO2', '_future_label', '_label_observable'],\n","      dtype='object')\n"]}],"source":["# 피처 정의 (LSTM도 동일 피처 사용)\n","# 기존 노트북에서 'ORIGINAL FEATURES ONLY' 의도대로: id/time/label/weight + 일부 파생지표는 제외\n","EXTRA_DROP = {\"calc_DiasBP\", \"ShockIndex\", \"PulsePressure\", \"ModShockIndex\", \"ROX_Index\",\"_label_observable\"}\n","DROP_COLS = {ID_COL, TIME_COL, \"event\", \"delta\", LABEL_COL} | (set([WEIGHT_COL]) if WEIGHT_COL else set()) | EXTRA_DROP\n","\n","FEATURE_COLS = [c for c in train_df.columns if c not in DROP_COLS]\n","if len(FEATURE_COLS) == 0:\n","    raise RuntimeError(\"피처 컬럼이 0개\")\n","\n","print(f\"n_features={len(FEATURE_COLS)}\")\n","print(FEATURE_COLS)\n","print(train_df.columns)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"HKaBoyFt2GW6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768636501022,"user_tz":-540,"elapsed":10956,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}},"outputId":"0883d10a-bf12-4e95-bf0a-7090406d9179"},"outputs":[{"output_type":"stream","name":"stdout","text":["Export dir: /content/artifacts/run_20260117_075449\n","Saved processed DFs: artifacts/run_20260117_075449/train_processed.parquet artifacts/run_20260117_075449/valid_processed.parquet artifacts/run_20260117_075449/test_processed.parquet\n","Saved meta.json\n","Saved boxhed_train_inputs.npz\n"]}],"source":["# =============================\n","# (추가) 학습 전: 학습 데이터/피처/전처리 산출물 추출\n","# - train/valid/test (전처리 완료 DF)\n","# - BoXHED 입력 행렬(X/y/w)\n","# - LSTM 입력(스케일러, 시퀀스/패딩)\n","# =============================\n","\n","import os, json\n","from pathlib import Path\n","\n","# 저장 위치\n","EXPORT_BASE = Path(\"./artifacts\")\n","EXPORT_BASE.mkdir(parents=True, exist_ok=True)\n","RUN_TAG = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n","OUT_DIR = EXPORT_BASE / f\"run_{RUN_TAG}\"\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","print(\"Export dir:\", OUT_DIR.resolve())\n","\n","# 1) 전처리 완료 데이터프레임 저장 (재현/검증 목적)\n","#   - 용량이 크면 parquet 권장. (로컬에서 parquet 엔진 미설치 시 csv로 대체)\n","\n","def _safe_to_parquet(df, path_base: Path):\n","    try:\n","        df.to_parquet(path_base.with_suffix('.parquet'), index=False)\n","        return str(path_base.with_suffix('.parquet'))\n","    except Exception as e:\n","        csv_path = path_base.with_suffix('.csv')\n","        df.to_csv(csv_path, index=False)\n","        print(f\"[WARN] parquet 저장 실패({e}). CSV로 저장: {csv_path}\")\n","        return str(csv_path)\n","\n","saved_train = _safe_to_parquet(train_df, OUT_DIR / \"train_processed\")\n","saved_valid = _safe_to_parquet(valid_df, OUT_DIR / \"valid_processed\")\n","saved_test  = _safe_to_parquet(test_df,  OUT_DIR / \"test_processed\")\n","print(\"Saved processed DFs:\", saved_train, saved_valid, saved_test)\n","\n","# 2) 피처/설정 메타데이터 저장\n","meta = {\n","    \"ID_COL\": ID_COL,\n","    \"TIME_COL\": TIME_COL,\n","    \"EVENT_COL\": EVENT_COL,\n","    \"WEIGHT_COL\": WEIGHT_COL,\n","    \"LABEL_COL\": LABEL_COL,\n","    \"FEATURE_COLS\": FEATURE_COLS,\n","    \"HORIZON_HOURS\": HORIZON_HOURS,\n","    \"DROP_AFTER_EVENT\": DROP_AFTER_EVENT,\n","    \"CUTOFF_HOURS\": CUTOFF_HOURS,\n","    \"AGG_MODE\": AGG_MODE,\n","    \"TARGET_RECALL\": TARGET_RECALL,\n","}\n","(Path(OUT_DIR) / \"meta.json\").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n","print(\"Saved meta.json\")\n","\n","# 3) BoXHED 학습 입력 저장\n","X_train_b = train_df[FEATURE_COLS].to_numpy(dtype=np.float32)\n","y_train_b = train_df[LABEL_COL].to_numpy(dtype=np.int64).reshape(-1)\n","\n","w_train_b = None\n","if WEIGHT_COL is not None and WEIGHT_COL in train_df.columns:\n","    w_train_b = train_df[WEIGHT_COL].to_numpy(dtype=np.float32).reshape(-1)\n","\n","# 클래스 가중 적용 전/후 모두 저장 (재현성)\n","w_train2 = apply_class_weight(w_train_b, y_train_b)\n","\n","np.savez_compressed(\n","    OUT_DIR / \"boxhed_train_inputs.npz\",\n","    X=X_train_b,\n","    y=y_train_b,\n","    w_raw=(w_train_b if w_train_b is not None else np.array([], dtype=np.float32)),\n","    w_applied=w_train2,\n",")\n","print(\"Saved boxhed_train_inputs.npz\")\n","\n","# # 4) LSTM 전처리 산출물 저장\n","# #   - scaler: train fit 후 valid/test transform에 사용\n","# #   - (옵션) 패딩된 X/y/sample_weight 저장 (용량이 커질 수 있음)\n","\n","# import joblib\n","# joblib.dump(scaler, OUT_DIR / \"lstm_scaler.joblib\")\n","# print(\"Saved lstm_scaler.joblib\")\n","\n","# SAVE_LSTM_ARRAYS = False  # 필요 시 True로 변경(대용량)\n","# if SAVE_LSTM_ARRAYS:\n","#     np.savez_compressed(\n","#         OUT_DIR / \"lstm_arrays.npz\",\n","#         X_tr=X_tr,\n","#         y_tr=y_tr,\n","#         sw_tr=sw_tr,\n","#         X_va=X_va,\n","#         y_va=y_va,\n","#         sw_va=sw_va,\n","#         X_te=X_te,\n","#         y_te=y_te,\n","#     )\n","#     print(\"Saved lstm_arrays.npz\")\n","\n","# print(\"[DONE] Pre-training artifact export\")\n"],"id":"HKaBoyFt2GW6"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"TxgBTAZh9HBf","executionInfo":{"status":"ok","timestamp":1768636581409,"user_tz":-540,"elapsed":1610,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}},"outputId":"76dea9c5-7dcf-4376-e751-aa0e2b34ea44","colab":{"base_uri":"https://localhost:8080/"}},"id":"TxgBTAZh9HBf","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"id":"261a1b91","metadata":{"id":"261a1b91","executionInfo":{"status":"aborted","timestamp":1768635860496,"user_tz":-540,"elapsed":318120,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["# ===============================\n","# Evaluation helpers (SELF-CONTAINED)\n","# ===============================\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, average_precision_score\n","\n","def safe_auc_ap(y_true, y_score):\n","    y_true = np.asarray(y_true).astype(int)\n","    y_score = np.asarray(y_score).astype(float)\n","    if len(np.unique(y_true)) < 2:\n","        return float('nan'), float('nan')\n","    return float(roc_auc_score(y_true, y_score)), float(average_precision_score(y_true, y_score))\n","\n","def metrics_at_threshold(y_true, y_score, thr):\n","    y_true = np.asarray(y_true).astype(int)\n","    y_score = np.asarray(y_score).astype(float)\n","    y_pred = (y_score >= float(thr)).astype(int)\n","    tp = int(((y_pred==1) & (y_true==1)).sum())\n","    fp = int(((y_pred==1) & (y_true==0)).sum())\n","    tn = int(((y_pred==0) & (y_true==0)).sum())\n","    fn = int(((y_pred==0) & (y_true==1)).sum())\n","    prec = tp / (tp + fp) if (tp+fp)>0 else 0.0\n","    rec  = tp / (tp + fn) if (tp+fn)>0 else 0.0\n","    return {\n","        'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn,\n","        'precision': float(prec),\n","        'recall': float(rec),\n","    }\n","\n","def pick_threshold_by_recall_then_precision(y_true, y_score, target_recall=0.8, n_grid=1001):\n","    y_true = np.asarray(y_true).astype(int)\n","    y_score = np.asarray(y_score).astype(float)\n","    # If all scores equal, threshold is that value\n","    lo, hi = float(np.nanmin(y_score)), float(np.nanmax(y_score))\n","    if not np.isfinite(lo) or not np.isfinite(hi) or lo==hi:\n","        thr = lo if np.isfinite(lo) else 0.5\n","        m = metrics_at_threshold(y_true, y_score, thr)\n","        return float(thr), m['precision'], m['recall']\n","    thrs = np.linspace(lo, hi, n_grid)\n","    best = None\n","    for thr in thrs:\n","        m = metrics_at_threshold(y_true, y_score, thr)\n","        if m['recall'] + 1e-12 >= float(target_recall):\n","            cand = (m['precision'], -thr, thr, m['recall'])\n","            if best is None or cand > best[0]:\n","                best = (cand, thr, m['precision'], m['recall'])\n","    if best is None:\n","        # can't reach target recall -> pick max recall then precision\n","        best2=None\n","        for thr in thrs:\n","            m=metrics_at_threshold(y_true,y_score,thr)\n","            cand=(m['recall'], m['precision'], -thr, thr)\n","            if best2 is None or cand>best2[0]:\n","                best2=(cand, thr, m['precision'], m['recall'])\n","        _,thr,prec,rec=best2\n","        return float(thr), float(prec), float(rec)\n","    _,thr,prec,rec=best\n","    return float(thr), float(prec), float(rec)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"97ed7cbc","metadata":{"id":"97ed7cbc","executionInfo":{"status":"aborted","timestamp":1768635860499,"user_tz":-540,"elapsed":318121,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["# =============================\n","# 1) BoXHED 학습/예측 (목표 정합 버전)\n","# - 학습 타깃: LABEL_COL (= (t, t+H] 미래 이벤트)\n","# - 예측: hazard -> sigmoid(hazard) 를 row_score로 사용\n","# =============================\n","\n","# BoXHED 학습 데이터\n","X_train_b = train_df[FEATURE_COLS].to_numpy(dtype=np.float32)\n","y_train_b = train_df[LABEL_COL].to_numpy(dtype=np.int64).reshape(-1)\n","\n","w_train_b = None\n","if WEIGHT_COL is not None and WEIGHT_COL in train_df.columns:\n","    w_train_b = train_df[WEIGHT_COL].to_numpy(dtype=np.float32).reshape(-1)\n","\n","w_train2 = apply_class_weight(w_train_b, y_train_b)\n","\n","boxhed_params = dict(max_depth=3, n_estimators=200, eta=0.05, min_child_events=1)\n","print(\"BoXHED params:\", boxhed_params)\n","\n","boxhed_model = boxhed(gpu_id=-1, nthread=8, **boxhed_params)\n","boxhed_model.fit(X_train_b, y_train_b, w_train2)\n","\n","# 평가 DF 기준 row_score 생성\n","valid_row_boxhed = predict_row_score_boxhed_df(boxhed_model, valid_df, FEATURE_COLS)\n","test_row_boxhed  = predict_row_score_boxhed_df(boxhed_model, test_df,  FEATURE_COLS)\n","\n","# stay-level 평가\n","yv_b, sv_b = stay_score_with_cutoff(valid_df, valid_row_boxhed, cutoff_hours=CUTOFF_HOURS, agg=AGG_MODE)\n","yt_b, st_b = stay_score_with_cutoff(test_df,  test_row_boxhed,  cutoff_hours=CUTOFF_HOURS, agg=AGG_MODE)\n","\n","thr_b, _, _ = pick_threshold_by_recall_then_precision(yv_b, sv_b, target_recall=TARGET_RECALL)\n","auc_v_b, ap_v_b = safe_auc_ap(yv_b, sv_b)\n","auc_t_b, ap_t_b = safe_auc_ap(yt_b, st_b)\n","\n","m_v_b = metrics_at_threshold(yv_b, sv_b, thr_b)\n","m_t_b = metrics_at_threshold(yt_b, st_b, thr_b)\n","\n","print(\"[BoXHED][VALID] AUC=%.4f AP=%.4f thr=%.4f P=%.4f R=%.4f\" % (auc_v_b, ap_v_b, thr_b, m_v_b[\"precision\"], m_v_b[\"recall\"]))\n","print(\"[BoXHED][TEST ] AUC=%.4f AP=%.4f thr=%.4f P=%.4f R=%.4f\" % (auc_t_b, ap_t_b, thr_b, m_t_b[\"precision\"], m_t_b[\"recall\"]))\n"]},{"cell_type":"code","execution_count":null,"id":"722d18c4","metadata":{"id":"722d18c4","executionInfo":{"status":"aborted","timestamp":1768635860502,"user_tz":-540,"elapsed":318123,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["# =============================\n","# 2) LSTM 학습/예측 (row_score) - GPU enabled, 조건 동일\n","# =============================\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from sklearn.preprocessing import StandardScaler\n","\n","# -------------------------------------------------\n","# GPU / cuDNN / XLA / mixed precision 설정 (조건 변경 아님)\n","# -------------------------------------------------\n","print(\"TF version:\", tf.__version__)\n","print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n","\n","# XLA (그래프 최적화)\n","tf.config.optimizer.set_jit(True)\n","\n","# mixed precision (GPU에서 속도 크게 개선, 연산 의미는 동일)\n","from tensorflow.keras import mixed_precision\n","mixed_precision.set_global_policy('mixed_float16')\n","print(\"Compute policy:\", mixed_precision.global_policy())\n","\n","# -------------------------------------------------\n","# 기존 코드 (로직/조건 동일)\n","# -------------------------------------------------\n","PAD_VALUE = -999.0\n","MAX_LEN = 120  # 시퀀스 최대 길이\n","\n","def make_sequences(df_long, feature_cols, id_col=ID_COL, time_col=TIME_COL):\n","    df_sorted = df_long.sort_values([id_col, time_col]).copy()\n","    X_list, y_list, idx_list = [], [], []\n","    for sid, g in df_sorted.groupby(id_col, sort=False):\n","        idx = g.index.to_numpy()\n","        X = g[feature_cols].to_numpy(dtype=np.float32)\n","        y = g[LABEL_COL].to_numpy(dtype=np.float32).reshape(-1, 1)\n","        X_list.append(X)\n","        y_list.append(y)\n","        idx_list.append(idx)\n","    return X_list, y_list, idx_list\n","\n","def pad_3d(X_list, max_len, pad_value=PAD_VALUE):\n","    F = X_list[0].shape[1]\n","    X_pad = np.full((len(X_list), max_len, F), pad_value, dtype=np.float32)\n","    mask = np.zeros((len(X_list), max_len), dtype=bool)\n","    for i, x in enumerate(X_list):\n","        L = min(x.shape[0], max_len)\n","        X_pad[i, :L, :] = x[:L]\n","        mask[i, :L] = True\n","    return X_pad, mask\n","\n","def pad_y(y_list, max_len):\n","    y_pad = np.zeros((len(y_list), max_len, 1), dtype=np.float32)\n","    for i, y in enumerate(y_list):\n","        L = min(y.shape[0], max_len)\n","        y_pad[i, :L, :] = y[:L]\n","    return y_pad\n","\n","def build_lstm_timewise(input_shape, pad_value=PAD_VALUE):\n","    inp = layers.Input(shape=input_shape)\n","    x = layers.Masking(mask_value=pad_value)(inp)\n","\n","    # cuDNN LSTM (GPU 자동 사용)\n","    x = layers.LSTM(64, return_sequences=True)(x)\n","    x = layers.Dropout(0.2)(x)\n","    x = layers.LSTM(32, return_sequences=True)(x)\n","\n","    # mixed precision 사용 시 출력은 float32로 고정 (수치 안정성)\n","    out = layers.TimeDistributed(\n","        layers.Dense(1, activation='sigmoid', dtype='float32')\n","    )(x)\n","\n","    model = models.Model(inp, out)\n","    model.compile(\n","        optimizer=tf.keras.optimizers.Adam(1e-3),\n","        loss='binary_crossentropy',\n","        metrics=[tf.keras.metrics.AUC(name='auc')]\n","    )\n","    return model\n","\n","# -------------------------------------------------\n","# scaling (train fit, valid/test transform)\n","# -------------------------------------------------\n","scaler = StandardScaler()\n","scaler.fit(train_df[FEATURE_COLS].to_numpy(dtype=np.float32))\n","\n","def scaled_copy(df):\n","    out = df.copy()\n","    out[FEATURE_COLS] = scaler.transform(out[FEATURE_COLS].to_numpy(dtype=np.float32))\n","    return out\n","\n","tr = scaled_copy(train_df)\n","va = scaled_copy(valid_df)\n","te = scaled_copy(test_df)\n","\n","# -------------------------------------------------\n","# sequence build + padding\n","# -------------------------------------------------\n","X_tr_list, y_tr_list, idx_tr_list = make_sequences(tr, FEATURE_COLS)\n","X_va_list, y_va_list, idx_va_list = make_sequences(va, FEATURE_COLS)\n","X_te_list, y_te_list, idx_te_list = make_sequences(te, FEATURE_COLS)\n","\n","X_tr, m_tr = pad_3d(X_tr_list, MAX_LEN)\n","X_va, m_va = pad_3d(X_va_list, MAX_LEN)\n","X_te, m_te = pad_3d(X_te_list, MAX_LEN)\n","\n","y_tr = pad_y(y_tr_list, MAX_LEN)\n","y_va = pad_y(y_va_list, MAX_LEN)\n","y_te = pad_y(y_te_list, MAX_LEN)\n","\n","# padding 구간 loss 제외\n","sw_tr = m_tr.astype(np.float32)\n","sw_va = m_va.astype(np.float32)\n","\n","# -------------------------------------------------\n","# 학습 (epochs / batch_size / patience 그대로)\n","# -------------------------------------------------\n","lstm_model = build_lstm_timewise((MAX_LEN, len(FEATURE_COLS)))\n","es = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_auc',\n","    mode='max',\n","    patience=8,\n","    restore_best_weights=True\n",")\n","\n","lstm_model.fit(\n","    X_tr, y_tr,\n","    validation_data=(X_va, y_va, sw_va),\n","    sample_weight=sw_tr,\n","    epochs=50,\n","    batch_size=256,\n","    callbacks=[es],\n","    verbose=1\n",")\n","\n","# -------------------------------------------------\n","# row_score 계산 (로직 동일)\n","# -------------------------------------------------\n","def predict_row_score_lstm(model, df_long_scaled, X_list, idx_list, max_len=MAX_LEN):\n","    X_pad, m = pad_3d(X_list, max_len)\n","    yhat = model.predict(X_pad, verbose=0).reshape(len(X_list), max_len)\n","\n","    row_score = np.full((len(df_long_scaled),), np.nan, dtype=np.float32)\n","    pos = {idx:i for i,idx in enumerate(df_long_scaled.index.to_numpy())}\n","\n","    for i, idx in enumerate(idx_list):\n","        L = min(len(idx), max_len)\n","        for j in range(L):\n","            row_score[pos[idx[j]]] = yhat[i, j]\n","    return row_score\n","\n","# long df는 원본 index 유지\n","tr_sorted = tr.sort_values([ID_COL, TIME_COL])\n","va_sorted = va.sort_values([ID_COL, TIME_COL])\n","te_sorted = te.sort_values([ID_COL, TIME_COL])\n","\n","valid_row_lstm = predict_row_score_lstm(lstm_model, va_sorted, X_va_list, idx_va_list)\n","test_row_lstm  = predict_row_score_lstm(lstm_model, te_sorted, X_te_list, idx_te_list)\n","\n","# -------------------------------------------------\n","# stay-level 평가\n","# -------------------------------------------------\n","yv_l, sv_l = stay_score_with_cutoff(va_sorted, valid_row_lstm, CUTOFF_HOURS, agg=AGG_MODE)\n","yt_l, st_l = stay_score_with_cutoff(te_sorted, test_row_lstm,  CUTOFF_HOURS, agg=AGG_MODE)\n","\n","thr_l, p_l, r_l = pick_threshold_by_recall_then_precision(\n","    yv_l, sv_l, target_recall=TARGET_RECALL\n",")\n","auc_v_l, ap_v_l = safe_auc_ap(yv_l, sv_l)\n","auc_t_l, ap_t_l = safe_auc_ap(yt_l, st_l)\n","\n","m_v_l = metrics_at_threshold(yv_l, sv_l, thr_l)\n","m_t_l = metrics_at_threshold(yt_l, st_l, thr_l)\n","\n","print(\"[LSTM][VALID] AUC=%.4f AP=%.4f thr=%.4f P=%.4f R=%.4f\"\n","      % (auc_v_l, ap_v_l, thr_l, m_v_l['precision'], m_v_l['recall']))\n","print(\"[LSTM][TEST ] AUC=%.4f AP=%.4f thr=%.4f P=%.4f R=%.4f\"\n","      % (auc_t_l, ap_t_l, thr_l, m_t_l['precision'], m_t_l['recall']))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGFIRpjS2GW9","executionInfo":{"status":"aborted","timestamp":1768635860504,"user_tz":-540,"elapsed":318125,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["# =============================\n","# (추가) 학습 후: 모델/추론 아티팩트 추출\n","# - BoXHED 모델 저장\n","# - LSTM 모델 저장\n","# - (옵션) 임계값(threshold) 및 평가 요약 저장\n","# =============================\n","\n","import json\n","from pathlib import Path\n","import joblib\n","\n","# pre-export 셀에서 만든 OUT_DIR을 재사용 (없으면 새로 생성)\n","try:\n","    OUT_DIR\n","except NameError:\n","    EXPORT_BASE = Path(\"./artifacts\")\n","    EXPORT_BASE.mkdir(parents=True, exist_ok=True)\n","    RUN_TAG = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n","    OUT_DIR = EXPORT_BASE / f\"run_{RUN_TAG}\"\n","    OUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","print(\"Export dir:\", OUT_DIR.resolve())\n","\n","# 1) BoXHED 모델 저장 (가능한 포맷을 순차 시도)\n","boxhed_saved = None\n","try:\n","    if hasattr(boxhed_model, \"save_model\"):\n","        path = OUT_DIR / \"boxhed_model.json\"\n","        boxhed_model.save_model(str(path))\n","        boxhed_saved = str(path)\n","    else:\n","        raise AttributeError(\"save_model not found\")\n","except Exception as e:\n","    # fallback: pickle/joblib\n","    try:\n","        path = OUT_DIR / \"boxhed_model.pkl\"\n","        joblib.dump(boxhed_model, path)\n","        boxhed_saved = str(path)\n","    except Exception as e2:\n","        print(\"[ERROR] BoXHED 모델 저장 실패:\", e, e2)\n","\n","print(\"Saved BoXHED model:\", boxhed_saved)\n","\n","# 2) LSTM 모델 저장\n","lstm_saved = None\n","try:\n","    # Keras 3 권장 포맷: .keras\n","    path = OUT_DIR / \"lstm_model.keras\"\n","    lstm_model.save(path)\n","    lstm_saved = str(path)\n","except Exception as e:\n","    # fallback: SavedModel\n","    try:\n","        path = OUT_DIR / \"lstm_savedmodel\"\n","        lstm_model.save(path)\n","        lstm_saved = str(path)\n","    except Exception as e2:\n","        print(\"[ERROR] LSTM 모델 저장 실패:\", e, e2)\n","\n","print(\"Saved LSTM model:\", lstm_saved)\n","\n","# 3) 운영/재현용 파라미터 저장 (threshold 등)\n","extra = {\n","    \"boxhed_threshold\": float(thr_b) if 'thr_b' in globals() else None,\n","    \"lstm_threshold\":  float(thr_l) if 'thr_l' in globals() else None,\n","    \"boxhed_params\":   boxhed_params if 'boxhed_params' in globals() else None,\n","    \"MAX_LEN\":         int(MAX_LEN) if 'MAX_LEN' in globals() else None,\n","    \"PAD_VALUE\":       float(PAD_VALUE) if 'PAD_VALUE' in globals() else None,\n","}\n","(Path(OUT_DIR) / \"trained_artifacts.json\").write_text(json.dumps(extra, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n","print(\"Saved trained_artifacts.json\")\n","\n","print(\"[DONE] Post-training model export\")\n"],"id":"eGFIRpjS2GW9"},{"cell_type":"code","execution_count":null,"id":"8c2e5109","metadata":{"id":"8c2e5109","executionInfo":{"status":"aborted","timestamp":1768635860507,"user_tz":-540,"elapsed":318127,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["# =============================\n","# 3) 결과 비교 요약\n","# =============================\n","\n","summary = pd.DataFrame([\n","    {\n","        \"model\": \"BoXHED\",\n","        \"valid_auc\": auc_v_b, \"valid_ap\": ap_v_b,\n","        \"valid_thr\": thr_b, \"valid_precision\": m_v_b[\"precision\"], \"valid_recall\": m_v_b[\"recall\"],\n","        \"test_auc\": auc_t_b, \"test_ap\": ap_t_b,\n","        \"test_precision\": m_t_b[\"precision\"], \"test_recall\": m_t_b[\"recall\"],\n","    },\n","    {\n","        \"model\": \"LSTM\",\n","        \"valid_auc\": auc_v_l, \"valid_ap\": ap_v_l,\n","        \"valid_thr\": thr_l, \"valid_precision\": m_v_l[\"precision\"], \"valid_recall\": m_v_l[\"recall\"],\n","        \"test_auc\": auc_t_l, \"test_ap\": ap_t_l,\n","        \"test_precision\": m_t_l[\"precision\"], \"test_recall\": m_t_l[\"recall\"],\n","    }\n","])\n","\n","summary\n"]},{"cell_type":"code","execution_count":null,"id":"65988530","metadata":{"id":"65988530","executionInfo":{"status":"aborted","timestamp":1768635860510,"user_tz":-540,"elapsed":318129,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["\n","# =============================\n","# 4) 누수/정합 빠른 점검\n","# =============================\n","\n","def check_disjoint(a, b, name_a, name_b, id_col=ID_COL):\n","    ia, ib = set(a[id_col].unique()), set(b[id_col].unique())\n","    inter = ia & ib\n","    print(f\"{name_a} stays: {len(ia):,} | {name_b} stays: {len(ib):,} | overlap: {len(inter):,}\")\n","    if inter:\n","        # 일부만 출력\n","        sample = list(inter)[:10]\n","        raise ValueError(f\"DATA LEAK: {name_a} and {name_b} share stay_id. sample={sample}\")\n","\n","check_disjoint(train_df, valid_df, \"train\", \"valid\")\n","check_disjoint(train_df, test_df,  \"train\", \"test\")\n","check_disjoint(valid_df, test_df,  \"valid\", \"test\")\n","\n","# 라벨/이벤트 이후 제거 확인\n","print(\"LABEL_COL=\", LABEL_COL, \"EVENT_COL=\", EVENT_COL)\n","print(\"Future-label positive rate (train/valid/test):\",\n","      train_df[LABEL_COL].mean().round(4),\n","      valid_df[LABEL_COL].mean().round(4),\n","      test_df[LABEL_COL].mean().round(4))\n","\n","# 이벤트 이후 제거가 동작했는지(이벤트가 있는 stay에서 t < first_event_t만 남는지) 샘플 체크\n","if DROP_AFTER_EVENT:\n","    tmp = test_df.copy()\n","    first_t = tmp.loc[tmp[EVENT_COL].astype(int)==1].groupby(ID_COL)[TIME_COL].min()\n","    if len(first_t)>0:\n","        # 제거 후에는 event row 자체가 없어야 정상 (t >= first_event_t 제거)\n","        assert tmp[EVENT_COL].sum() == 0, \"DROP_AFTER_EVENT=True인데 event==1 row가 남아있습니다.\"\n","        print(\"Post-event rows removed: OK (no event rows remain in filtered splits)\")\n"]},{"cell_type":"code","execution_count":null,"id":"40a5d4fc","metadata":{"id":"40a5d4fc","executionInfo":{"status":"aborted","timestamp":1768635860516,"user_tz":-540,"elapsed":318135,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["# === Leakage-Defense Checks (heuristics) ===\n","# 목적:\n","# 1) 피처명에 outcome/future/label 등 누수 토큰이 포함된 경우 경고\n","# 2) 시간 누수 가능성이 큰 \"정산형/누적형\" 피처(cum/total/elapsed 등) 후보를 목록화\n","#\n","# 주의: 이 셀은 \"누수 가능성\"을 자동으로 찾아주는 휴리스틱입니다.\n","#      경고가 뜬다고 100% 누수라는 뜻은 아니고, 사람이 확인해야 합니다.\n","\n","import re\n","import numpy as np\n","import pandas as pd\n","\n","def _g(name):\n","    return globals().get(name, None)\n","\n","train_df = _g('train_df')\n","FEATURE_COLS = _g('FEATURE_COLS')\n","\n","if train_df is None or FEATURE_COLS is None:\n","    print('Leakage-Defense Checks: train_df / FEATURE_COLS 가 아직 정의되지 않았습니다. (위 셀을 먼저 실행하세요)')\n","else:\n","    feature_cols = list(FEATURE_COLS)\n","\n","    # ---- 1) Name-based leakage token scan ----\n","    # 이름에 아래 토큰이 포함되어 있으면 결과/미래/라벨 유입 가능성이 있습니다.\n","    # 정상 피처일 수도 있지만, 반드시 사람이 확인하세요.\n","    leak_tokens = [\n","        r'label', r'target', r'outcome', r'event', r'delta',\n","        r'mort', r'death', r'expire', r'discharge', r'dod',\n","        r'future', r'next', r'horizon', r'\\btt[e|d]\\b',\n","        r'readmit', r'\\blos\\b', r'length[_\\s]*of[_\\s]*stay',\n","        r'\\by\\b$', r'\\by_', r'_y$',\n","    ]\n","    leak_pat = re.compile('|'.join(leak_tokens), flags=re.IGNORECASE)\n","    name_flagged = [c for c in feature_cols if leak_pat.search(str(c))]\n","\n","    print('\\n[1] Name-based leakage token scan')\n","    if name_flagged:\n","        print(f'  ⚠️  Suspicious feature name(s): {len(name_flagged)}')\n","        for c in name_flagged[:80]:\n","            print('   -', c)\n","        if len(name_flagged) > 80:\n","            print(f'   ...(and {len(name_flagged)-80} more)')\n","        print('  Action: 해당 피처가 시점 t에서 이용 가능한 정보만으로 계산됐는지 확인하세요.')\n","    else:\n","        print('  ✅ No suspicious tokens found in FEATURE_COLS')\n","\n","    # ---- 2) Accumulator-style feature name scan ----\n","    acc_tokens = [\n","        r'cum', r'cumulative', r'total', r'sum', r'count',\n","        r'elapsed', r'since', r'duration', r'time_since',\n","        r'ever_', r'history', r'lifetime', r'to_date',\n","        r'max_ever', r'min_ever',\n","    ]\n","    acc_pat = re.compile('|'.join(acc_tokens), flags=re.IGNORECASE)\n","    acc_flagged = [c for c in feature_cols if acc_pat.search(str(c))]\n","\n","    print('\\n[2] Accumulator-style feature name scan')\n","    if acc_flagged:\n","        print(f'  ⚠️  Accumulator-like feature name(s): {len(acc_flagged)}')\n","        for c in acc_flagged[:80]:\n","            print('   -', c)\n","        if len(acc_flagged) > 80:\n","            print(f'   ...(and {len(acc_flagged)-80} more)')\n","        print('  Action: 누적/정산 피처가 \"t 이전 정보만\" 사용해 계산됐는지 확인하세요.')\n","    else:\n","        print('  ✅ No accumulator-like tokens found in FEATURE_COLS')\n","\n","    # ---- 3) Monotonicity heuristic within stay (numeric only) ----\n","    print('\\n[3] Monotonicity heuristic within stay (numeric only)')\n","    if 'stay_id' not in train_df.columns or 't' not in train_df.columns:\n","        print('  (skip) stay_id / t 컬럼이 없어 단조성 점검을 생략합니다.')\n","    else:\n","        df_m = train_df[['stay_id','t'] + [c for c in feature_cols if c in train_df.columns]].copy()\n","        # sample stays for speed\n","        try:\n","            uniq = df_m['stay_id'].unique()\n","            if len(uniq) > 2000:\n","                sampled = np.random.choice(uniq, size=2000, replace=False)\n","                df_m = df_m[df_m['stay_id'].isin(sampled)]\n","        except Exception:\n","            pass\n","\n","        num_cols = [c for c in feature_cols if c in df_m.columns and pd.api.types.is_numeric_dtype(df_m[c])]\n","        if not num_cols:\n","            print('  (skip) numeric feature column이 없어 단조성 점검을 생략합니다.')\n","        else:\n","            df_m = df_m.sort_values(['stay_id','t'])\n","\n","            def _monotone_score(g, col):\n","                x = g[col].to_numpy(dtype=float)\n","                x = x[~np.isnan(x)]\n","                if len(x) < 3:\n","                    return np.nan\n","                dx = np.diff(x)\n","                inc = np.mean(dx >= -1e-8)\n","                dec = np.mean(dx <=  1e-8)\n","                return float(max(inc, dec))\n","\n","            eval_cols = num_cols[:300]  # cap for speed\n","            rows = []\n","            gb = df_m.groupby('stay_id', sort=False)\n","            for col in eval_cols:\n","                s = gb.apply(lambda g: _monotone_score(g, col)).mean()\n","                rows.append((col, float(s)))\n","\n","            mono = pd.DataFrame(rows, columns=['feature','mean_monotone_score']).sort_values('mean_monotone_score', ascending=False)\n","            strong = mono[mono['mean_monotone_score'] >= 0.98]\n","            if strong.empty:\n","                print('  ✅ No strong monotone candidates found (threshold: 0.98)')\n","                print('  Note: 이건 휴리스틱이라, 누수가 없다는 의미는 아닙니다.')\n","            else:\n","                print(f'  ⚠️  Strong monotone candidates (>=0.98): {len(strong)}')\n","                try:\n","                    display(strong.head(30))\n","                except Exception:\n","                    print(strong.head(30).to_string(index=False))\n","                print('  Action: 해당 피처가 누적/경과시간/정산형이면, 계산에 미래값이 섞이지 않았는지 확인하세요.')\n"]},{"cell_type":"code","execution_count":null,"id":"88376345","metadata":{"id":"88376345","executionInfo":{"status":"aborted","timestamp":1768635860519,"user_tz":-540,"elapsed":318137,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["# =============================\n","# LSTM helpers for Horizon sweep (self-contained)\n","# - Cell 14에서 NameError 없이 동작하도록 함수 형태로 제공\n","# =============================\n","\n","from sklearn.preprocessing import StandardScaler\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","\n","PAD_VALUE = -999.0\n","\n","def _make_sequences(df_long, feature_cols, id_col, time_col, label_col):\n","    df_sorted = df_long.sort_values([id_col, time_col]).copy()\n","    X_list, y_list, idx_list = [], [], []\n","    for _, g in df_sorted.groupby(id_col, sort=False):\n","        idx = g.index.to_numpy()\n","        X = g[feature_cols].to_numpy(dtype=np.float32)\n","        y = g[label_col].to_numpy(dtype=np.float32).reshape(-1, 1)\n","        X_list.append(X); y_list.append(y); idx_list.append(idx)\n","    return df_sorted, X_list, y_list, idx_list\n","\n","def _pad_3d(X_list, max_len, pad_value=PAD_VALUE):\n","    F = X_list[0].shape[1]\n","    X_pad = np.full((len(X_list), max_len, F), pad_value, dtype=np.float32)\n","    mask = np.zeros((len(X_list), max_len), dtype=bool)\n","    for i, x in enumerate(X_list):\n","        L = min(x.shape[0], max_len)\n","        X_pad[i, :L, :] = x[:L]\n","        mask[i, :L] = True\n","    return X_pad, mask\n","\n","def _pad_y(y_list, max_len):\n","    y_pad = np.zeros((len(y_list), max_len, 1), dtype=np.float32)\n","    for i, y in enumerate(y_list):\n","        L = min(y.shape[0], max_len)\n","        y_pad[i, :L, :] = y[:L]\n","    return y_pad\n","\n","def build_lstm_timewise(input_shape, pad_value=PAD_VALUE):\n","    inp = layers.Input(shape=input_shape)\n","    x = layers.Masking(mask_value=pad_value)(inp)\n","    x = layers.LSTM(64, return_sequences=True)(x)\n","    x = layers.Dropout(0.2)(x)\n","    x = layers.LSTM(32, return_sequences=True)(x)\n","    out = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(x)\n","    m = models.Model(inp, out)\n","    m.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n","              loss='binary_crossentropy',\n","              metrics=[tf.keras.metrics.AUC(name='auc')])\n","    return m\n","\n","def train_lstm_row_model(train_df, valid_df, feature_cols, id_col, time_col, label_col, max_len=120):\n","    scaler = StandardScaler()\n","    scaler.fit(train_df[feature_cols].to_numpy(dtype=np.float32))\n","\n","    tr = train_df.copy(); va = valid_df.copy()\n","    tr[feature_cols] = scaler.transform(tr[feature_cols].to_numpy(dtype=np.float32))\n","    va[feature_cols] = scaler.transform(va[feature_cols].to_numpy(dtype=np.float32))\n","\n","    _, X_tr_list, y_tr_list, _ = _make_sequences(tr, feature_cols, id_col, time_col, label_col)\n","    _, X_va_list, y_va_list, _ = _make_sequences(va, feature_cols, id_col, time_col, label_col)\n","\n","    X_tr, m_tr = _pad_3d(X_tr_list, max_len)\n","    X_va, m_va = _pad_3d(X_va_list, max_len)\n","    y_tr = _pad_y(y_tr_list, max_len)\n","    y_va = _pad_y(y_va_list, max_len)\n","\n","    sw_tr = m_tr.astype(np.float32)\n","    sw_va = m_va.astype(np.float32)\n","\n","    model = build_lstm_timewise((max_len, len(feature_cols)))\n","    es = tf.keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=5, restore_best_weights=True, verbose=0)\n","    model.fit(X_tr, y_tr, validation_data=(X_va, y_va, sw_va), sample_weight=sw_tr,\n","              epochs=30, batch_size=256, callbacks=[es], verbose=0)\n","    return model, scaler\n","\n","def predict_row_score_lstm(model, scaler, df_eval, feature_cols, id_col, time_col, max_len=120):\n","    # row_score는 df_eval '원본 행 순서'에 정렬되도록 반환\n","    assert df_eval.index.is_unique, 'df_eval index must be unique for safe mapping'\n","    df_scaled = df_eval.copy()\n","    df_scaled[feature_cols] = scaler.transform(df_scaled[feature_cols].to_numpy(dtype=np.float32))\n","\n","    df_sorted, X_list, _, idx_list = _make_sequences(df_scaled, feature_cols, id_col, time_col, label_col=LABEL_COL)\n","    X_pad, _ = _pad_3d(X_list, max_len)\n","    yhat = model.predict(X_pad, verbose=0).reshape(len(X_list), max_len)\n","\n","    pos = {idx: i for i, idx in enumerate(df_eval.index.to_numpy())}\n","    row_score = np.full((len(df_eval),), np.nan, dtype=np.float32)\n","    for i, idx in enumerate(idx_list):\n","        L = min(len(idx), max_len)\n","        for j in range(L):\n","            row_score[pos[idx[j]]] = yhat[i, j]\n","    if np.isnan(row_score).any():\n","        # padding 구간 외 NaN이 남으면 매핑 실패 가능성\n","        pass\n","    return row_score\n"]},{"cell_type":"code","execution_count":null,"id":"1f724bb5","metadata":{"id":"1f724bb5","executionInfo":{"status":"aborted","timestamp":1768635860523,"user_tz":-540,"elapsed":318140,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["# =============================\n","# 5) Horizon별 성능 곡선 (BoXHED vs LSTM) - 목표 정합 버전\n","# =============================\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","HORIZONS_TO_EVAL = [6, 12, 24]  # hours\n","\n","def _load_raw():\n","    tr = pd.read_csv(train_path)\n","    va = pd.read_csv(valid_path)\n","    te = pd.read_csv(test_path)\n","    return tr, va, te\n","\n","def _prep_eval_dfs(tr_raw, va_raw, te_raw, horizon_hours: int):\n","    # 1) 미래 라벨 생성\n","    tr = add_future_label(tr_raw.copy(), horizon_hours=horizon_hours)\n","    va = add_future_label(va_raw.copy(), horizon_hours=horizon_hours)\n","    te = add_future_label(te_raw.copy(), horizon_hours=horizon_hours)\n","\n","    # 2) 검열 마스크 → 관측 불가능 row 제거\n","    tr = add_label_observable_mask(tr, horizon_hours=horizon_hours)\n","    va = add_label_observable_mask(va, horizon_hours=horizon_hours)\n","    te = add_label_observable_mask(te, horizon_hours=horizon_hours)\n","\n","    tr = tr.loc[tr[\"_label_observable\"]].copy()\n","    va = va.loc[va[\"_label_observable\"]].copy()\n","    te = te.loc[te[\"_label_observable\"]].copy()\n","\n","    # 3) 이벤트 이후 제거\n","    if DROP_AFTER_EVENT:\n","        tr = drop_rows_after_first_event(tr)\n","        va = drop_rows_after_first_event(va)\n","        te = drop_rows_after_first_event(te)\n","\n","    # 4) 옵션: train cutoff\n","    if APPLY_CUTOFF_TO_TRAIN:\n","        tr = tr.loc[tr[TIME_COL] <= CUTOFF_HOURS].copy()\n","\n","    return tr, va, te\n","\n","def _feature_cols_from(df: pd.DataFrame):\n","    # 미정의 변수(DELTA_COL 등) 제거: 고정 drop set 사용\n","    drop = {ID_COL, TIME_COL, LABEL_COL, \"event\", \"delta\"}\n","    if WEIGHT_COL is not None:\n","        drop.add(WEIGHT_COL)\n","    # 파생 지표 제외(노트북 기본 의도 유지)\n","    drop |= {\"calc_DiasBP\", \"ShockIndex\", \"PulsePressure\", \"ModShockIndex\", \"ROX_Index\"}\n","    cols = [c for c in df.columns if c not in drop]\n","    if len(cols) == 0:\n","        raise RuntimeError(\"No feature columns after dropping.\")\n","    return cols\n","\n","def _run_boxhed_one(tr_eval, va_eval, te_eval, feature_cols):\n","    X_tr = tr_eval[feature_cols].to_numpy(dtype=np.float32)\n","    y_tr = tr_eval[LABEL_COL].to_numpy(dtype=np.int64).reshape(-1)\n","\n","    w_tr = None\n","    if WEIGHT_COL is not None and WEIGHT_COL in tr_eval.columns:\n","        w_tr = tr_eval[WEIGHT_COL].to_numpy(dtype=np.float32).reshape(-1)\n","    w_tr2 = apply_class_weight(w_tr, y_tr)\n","\n","    params = dict(max_depth=3, n_estimators=200, eta=0.05, min_child_events=1)\n","    model = boxhed(gpu_id=-1, nthread=8, **params)\n","    model.fit(X_tr, y_tr, w_tr2)\n","\n","    v_row = predict_row_score_boxhed_df(model, va_eval, feature_cols)\n","    t_row = predict_row_score_boxhed_df(model, te_eval, feature_cols)\n","    return v_row, t_row\n","\n","def _run_lstm_one(tr_eval, va_eval, te_eval, feature_cols):\n","    lstm_model, scaler = train_lstm_row_model(\n","        train_df=tr_eval,\n","        valid_df=va_eval,\n","        feature_cols=feature_cols,\n","        id_col=ID_COL,\n","        time_col=TIME_COL,\n","        label_col=LABEL_COL,\n","        max_len=MAX_LEN,\n","    )\n","    v_row = predict_row_score_lstm(lstm_model, scaler, va_eval, feature_cols, id_col=ID_COL, time_col=TIME_COL, max_len=MAX_LEN)\n","    t_row = predict_row_score_lstm(lstm_model, scaler, te_eval, feature_cols, id_col=ID_COL, time_col=TIME_COL, max_len=MAX_LEN)\n","    return v_row, t_row\n","\n","def _eval_one(df_eval, row_score):\n","    return stay_score_with_cutoff(df_eval, row_score, cutoff_hours=CUTOFF_HOURS, agg=AGG_MODE)\n","\n","rows = []\n","for H in HORIZONS_TO_EVAL:\n","    print(f\"===== Horizon {H}h =====\")\n","    tr_raw, va_raw, te_raw = _load_raw()\n","    tr_eval, va_eval, te_eval = _prep_eval_dfs(tr_raw, va_raw, te_raw, H)\n","\n","    # split 누수 체크\n","    check_disjoint(tr_eval, va_eval, \"train\", \"valid\")\n","    check_disjoint(tr_eval, te_eval, \"train\", \"test\")\n","    check_disjoint(va_eval, te_eval, \"valid\", \"test\")\n","\n","    feature_cols = _feature_cols_from(tr_eval)\n","\n","    # --- BoXHED ---\n","    v_row_b, t_row_b = _run_boxhed_one(tr_eval, va_eval, te_eval, feature_cols)\n","    yv_b, sv_b = _eval_one(va_eval, v_row_b)\n","    yt_b, st_b = _eval_one(te_eval, t_row_b)\n","    thr_b, _, _ = pick_threshold_by_recall_then_precision(yv_b, sv_b, target_recall=TARGET_RECALL)\n","    auc_v_b, ap_v_b = safe_auc_ap(yv_b, sv_b)\n","    auc_t_b, ap_t_b = safe_auc_ap(yt_b, st_b)\n","    mv_b = metrics_at_threshold(yv_b, sv_b, thr_b)\n","    mt_b = metrics_at_threshold(yt_b, st_b, thr_b)\n","\n","    rows.append(dict(model=\"BoXHED\", horizon_hours=H,\n","                     valid_auc=auc_v_b, valid_ap=ap_v_b, valid_precision=mv_b[\"precision\"], valid_recall=mv_b[\"recall\"],\n","                     test_auc=auc_t_b,  test_ap=ap_t_b,  test_precision=mt_b[\"precision\"],  test_recall=mt_b[\"recall\"]))\n","\n","    # --- LSTM ---\n","    v_row_l, t_row_l = _run_lstm_one(tr_eval, va_eval, te_eval, feature_cols)\n","    yv_l, sv_l = _eval_one(va_eval, v_row_l)\n","    yt_l, st_l = _eval_one(te_eval, t_row_l)\n","    thr_l, _, _ = pick_threshold_by_recall_then_precision(yv_l, sv_l, target_recall=TARGET_RECALL)\n","    auc_v_l, ap_v_l = safe_auc_ap(yv_l, sv_l)\n","    auc_t_l, ap_t_l = safe_auc_ap(yt_l, st_l)\n","    mv_l = metrics_at_threshold(yv_l, sv_l, thr_l)\n","    mt_l = metrics_at_threshold(yt_l, st_l, thr_l)\n","\n","    rows.append(dict(model=\"LSTM\", horizon_hours=H,\n","                     valid_auc=auc_v_l, valid_ap=ap_v_l, valid_precision=mv_l[\"precision\"], valid_recall=mv_l[\"recall\"],\n","                     test_auc=auc_t_l,  test_ap=ap_t_l,  test_precision=mt_l[\"precision\"],  test_recall=mt_l[\"recall\"]))\n","\n","df_h = pd.DataFrame(rows)\n","display(df_h)\n","\n","def _plot(metric, split=\"test\"):\n","    plt.figure()\n","    for model_name in [\"BoXHED\", \"LSTM\"]:\n","        sub = df_h[df_h[\"model\"] == model_name].sort_values(\"horizon_hours\")\n","        plt.plot(sub[\"horizon_hours\"], sub[f\"{split}_{metric}\"], marker=\"o\", label=model_name)\n","    plt.xlabel(\"Horizon (hours)\")\n","    plt.ylabel(f\"{split}.{metric}\")\n","    plt.title(f\"{split}.{metric} vs Horizon\")\n","    plt.grid(True)\n","    plt.legend()\n","    plt.show()\n","\n","for m in [\"auc\", \"ap\", \"precision\", \"recall\"]:\n","    _plot(m, split=\"test\")\n"]},{"cell_type":"markdown","id":"14c8a089","metadata":{"id":"14c8a089"},"source":["## Time-dependent evaluation (row-level): AUC/AP by time t"]},{"cell_type":"code","execution_count":null,"id":"cc8d71c0","metadata":{"id":"cc8d71c0","executionInfo":{"status":"aborted","timestamp":1768635860528,"user_tz":-540,"elapsed":318145,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","def _safe_auc_ap(y, s):\n","    y = np.asarray(y).astype(int)\n","    s = np.asarray(s).astype(float)\n","    if len(np.unique(y)) < 2:\n","        return np.nan, np.nan\n","    return safe_auc_ap(y, s)  # 노트북 기존 함수 사용\n","\n","def time_slice_metrics(df_long, row_score, time_col=TIME_COL, label_col=LABEL_COL,\n","                       cutoff_hours=CUTOFF_HOURS, time_mode=\"exact_int\"):\n","    \"\"\"시간별(row-level) AUC/AP 계산.\"\"\"\n","    d = df_long.copy()\n","    d[\"_row_score\"] = np.asarray(row_score, dtype=np.float32)\n","\n","    if time_mode == \"exact_int\":\n","        d[\"_tbin\"] = d[time_col].astype(int)\n","    elif time_mode == \"bin_floor\":\n","        d[\"_tbin\"] = np.floor(d[time_col].to_numpy(dtype=float)).astype(int)\n","    else:\n","        raise ValueError(\"time_mode must be one of {'exact_int','bin_floor'}\")\n","\n","    d = d.loc[(d[\"_tbin\"] >= 0) & (d[\"_tbin\"] <= cutoff_hours)].copy()\n","\n","    rows = []\n","    for t in range(0, cutoff_hours + 1):\n","        sub = d.loc[d[\"_tbin\"] == t]\n","        if len(sub) == 0:\n","            rows.append(dict(t=t, n=0, pos=0, auc=np.nan, ap=np.nan))\n","            continue\n","        y = sub[label_col].to_numpy(dtype=int)\n","        s = sub[\"_row_score\"].to_numpy(dtype=float)\n","        auc, ap = _safe_auc_ap(y, s)\n","        rows.append(dict(t=t, n=len(sub), pos=int((y == 1).sum()), auc=auc, ap=ap))\n","\n","    out = pd.DataFrame(rows)\n","    out[\"pos_rate\"] = out[\"pos\"] / out[\"n\"].replace(0, np.nan)\n","    return out\n","\n","def plot_compare(df_time, metric, split_name=\"TEST\"):\n","    plt.figure()\n","    for model_name in [\"BoXHED\", \"LSTM\"]:\n","        sub = df_time[df_time[\"model\"] == model_name].sort_values(\"t\")\n","        plt.plot(sub[\"t\"], sub[metric], marker=\"o\", label=model_name)\n","    plt.xlabel(\"time t (hours)\")\n","    plt.ylabel(metric)\n","    plt.title(f\"{split_name} time-dependent {metric}\")\n","    plt.grid(True)\n","    plt.legend()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"3f46f11b","metadata":{"id":"3f46f11b","executionInfo":{"status":"aborted","timestamp":1768635860532,"user_tz":-540,"elapsed":318148,"user":{"displayName":"10 KDT","userId":"07089621398428817703"}}},"outputs":[],"source":["# ====== Time-dependent evaluation (row-level) ======\n","# BoXHED row scores (위에서 생성된 것)\n","v_b = valid_row_boxhed\n","t_b = test_row_boxhed\n","\n","# LSTM row scores: 노트북 변수명이 다르면 아래 두 줄만 수정하세요.\n","# (일반적으로 valid_row_lstm / test_row_lstm 형태)\n","v_l = valid_row_lstm\n","t_l = test_row_lstm\n","\n","# TIME_COL이 정수 시간이라면 \"exact_int\", 실수/불규칙이면 \"bin_floor\"\n","TIME_MODE = \"exact_int\"\n","\n","# ---- VALID ----\n","m_v_b = time_slice_metrics(valid_df, v_b, cutoff_hours=CUTOFF_HOURS, time_mode=TIME_MODE)\n","m_v_l = time_slice_metrics(valid_df, v_l, cutoff_hours=CUTOFF_HOURS, time_mode=TIME_MODE)\n","\n","m_v_b[\"model\"] = \"BoXHED\"\n","m_v_l[\"model\"] = \"LSTM\"\n","df_time_valid = pd.concat([m_v_b, m_v_l], axis=0, ignore_index=True)\n","display(df_time_valid)\n","\n","# ---- TEST ----\n","m_t_b = time_slice_metrics(test_df, t_b, cutoff_hours=CUTOFF_HOURS, time_mode=TIME_MODE)\n","m_t_l = time_slice_metrics(test_df, t_l, cutoff_hours=CUTOFF_HOURS, time_mode=TIME_MODE)\n","\n","m_t_b[\"model\"] = \"BoXHED\"\n","m_t_l[\"model\"] = \"LSTM\"\n","df_time_test = pd.concat([m_t_b, m_t_l], axis=0, ignore_index=True)\n","display(df_time_test)\n","\n","for metric in [\"auc\", \"ap\", \"pos_rate\"]:\n","    plot_compare(df_time_test, metric, split_name=\"TEST\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}